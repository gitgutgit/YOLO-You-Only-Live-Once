"""
AI Module - Reinforcement Learning Policy

Chloe Lee (cl4490) ë‹´ë‹¹ ëª¨ë“ˆ
PPO/DQN ê¸°ë°˜ ê²Œì„ AI ì •ì±…

TODO for Chloe:
1. simulate_ai_decision() â†’ real_ppo_decision() êµì²´
2. ì •ì±… ë„¤íŠ¸ì›Œí¬ í›ˆë ¨ ë° ë¡œë“œ
3. ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì • ìµœì í™”
4. ìê°€ í•™ìŠµ (Self-Play) êµ¬í˜„
"""
from src.models.policy_network import PolicyNetwork, ValueNetwork

import numpy as np
from typing import Dict, List, Tuple, Optional, Any
import time
import random

# PyTorchëŠ” ì„ íƒì  (ì‹¤ì œ RL ëª¨ë¸ êµ¬í˜„ ì‹œ í•„ìš”)
try:
    import torch
    import torch.nn as nn
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False
    print("âš ï¸ PyTorch (torch) ì—†ìŒ - ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë§Œ ì‚¬ìš© ê°€ëŠ¥")
    # ë”ë¯¸ í´ë˜ìŠ¤ (íƒ€ì… íŒíŠ¸ìš©)
    class nn:
        class Module:
            pass
        class Sequential:
            pass
        class Linear:
            pass
        class ReLU:
            pass
        class Softmax:
            pass

# TODO: Chloeê°€ ì¶”ê°€í•  import
# from stable_baselines3 import PPO, DQN
# from ..src.utils.rl_instrumentation import RLInstrumentationLogger


class PolicyNetwork(nn.Module):
    """
    ì •ì±… ë„¤íŠ¸ì›Œí¬ (MLP)
    
    Chloeê°€ êµ¬í˜„í•  ì‹ ê²½ë§ êµ¬ì¡°
    """
    
    def __init__(self, state_dim: int = 8, hidden_dim: int = 128, action_dim: int = 4):
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch (torch)ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‹¤ì œ RL ëª¨ë¸ êµ¬í˜„ ì‹œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim),
            nn.Softmax(dim=-1)
        )
    
    def forward(self, state):
        return self.network(state)


class ValueNetwork(nn.Module):
    """
    ê°€ì¹˜ ë„¤íŠ¸ì›Œí¬ (PPOìš©)
    
    Chloeê°€ PPO êµ¬í˜„ ì‹œ ì‚¬ìš©
    """
    
    def __init__(self, state_dim: int = 8, hidden_dim: int = 128):
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorch (torch)ê°€ í•„ìš”í•©ë‹ˆë‹¤. ì‹¤ì œ RL ëª¨ë¸ êµ¬í˜„ ì‹œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
        super().__init__()
        
        self.network = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, 1)
        )
    
    def forward(self, state):
        return self.network(state)


class AIDecisionResult:
    """AI ì˜ì‚¬ê²°ì • ê²°ê³¼"""
    
    def __init__(self, action: str, confidence: float, reasoning: str = "", 
                 action_probs: Optional[Dict[str, float]] = None):
        self.action = action
        self.confidence = confidence
        self.reasoning = reasoning
        self.action_probs = action_probs or {}
        self.timestamp = time.time()
    
    def to_dict(self) -> Dict[str, Any]:
        """ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜ (ì›¹ ì „ì†¡ìš©)"""
        return {
            'action': self.action,
            'confidence': self.confidence,
            'reasoning': self.reasoning,
            'action_probs': self.action_probs,
            'timestamp': self.timestamp
        }


class AIModule:
    """
    AI ëª¨ë“ˆ - ê°•í™”í•™ìŠµ ê¸°ë°˜ ê²Œì„ AI
    
    Chloeê°€ êµ¬í˜„í•  ì£¼ìš” ê¸°ëŠ¥:
    1. PPO/DQN ì •ì±… ë¡œë“œ ë° ì¶”ë¡ 
    2. ì‹¤ì‹œê°„ ì˜ì‚¬ê²°ì •
    3. ìê°€ í•™ìŠµ ë°ì´í„° ìˆ˜ì§‘
    4. ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    """
    
    def __init__(self, model_path: Optional[str] = None, algorithm: str = "PPO"):
        """
        ì´ˆê¸°í™”
        
        Args:
            model_path: í›ˆë ¨ëœ ëª¨ë¸ ê²½ë¡œ
            algorithm: ì‚¬ìš©í•  ì•Œê³ ë¦¬ì¦˜ ("PPO" ë˜ëŠ” "DQN")
        """
        self.model_path = model_path
        self.algorithm = algorithm
        # PyTorchê°€ ì—†ìœ¼ë©´ deviceëŠ” None (ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ)
        if TORCH_AVAILABLE:
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        else:
            self.device = None
        
        # ëª¨ë¸ë“¤
        self.policy_net = None
        self.value_net = None
        self.ppo_model = None
        self.dqn_model = None
        
        # ì„±ëŠ¥ ì¶”ì 
        self.decision_times = []
        self.action_history = []
        self.reward_history = []
        
        # RL ê³„ì¸¡ (Chloeê°€ êµ¬í˜„)
        self.rl_logger = None
        
        # ì´ˆê¸°í™”
        self._initialize_model()
    
    def _initialize_model(self):




        self.policy_net = PolicyNetwork().to(self.device)
        self.value_net = ValueNetwork().to(self.device)

        """
        ëª¨ë¸ ì´ˆê¸°í™”
        
        TODO for Chloe: ì‹¤ì œ PPO/DQN ëª¨ë¸ ë¡œë“œ êµ¬í˜„
        """
        if self.model_path:
            # TODO: ì‹¤ì œ êµ¬í˜„
            # if self.algorithm == "PPO":
            #     self.ppo_model = PPO.load(self.model_path)
            # elif self.algorithm == "DQN":
            #     self.dqn_model = DQN.load(self.model_path)
            
            print(f"ğŸ¤– [Chloe TODO] {self.algorithm} ëª¨ë¸ ë¡œë“œ: {self.model_path}")
        else:
            # ê¸°ë³¸ ì •ì±… ë„¤íŠ¸ì›Œí¬ (ì‹œë®¬ë ˆì´ì…˜ìš©) - PyTorchê°€ ìˆì„ ë•Œë§Œ
            if TORCH_AVAILABLE:
                self.policy_net = PolicyNetwork().to(self.device)
            print("âš ï¸ ëª¨ë¸ ê²½ë¡œê°€ ì—†ìŠµë‹ˆë‹¤. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤.")
        
        # RL ê³„ì¸¡ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        # TODO: self.rl_logger = RLInstrumentationLogger("web_game_ai")
    
    def make_decision(self, game_state: Dict[str, Any]) -> AIDecisionResult:
        """
        ê²Œì„ ìƒíƒœë¥¼ ë³´ê³  í–‰ë™ ê²°ì •
        
        Args:
            game_state: ê²Œì„ ì—”ì§„ì—ì„œ ë°›ì€ ìƒíƒœ ì •ë³´
            
        Returns:
            AI ì˜ì‚¬ê²°ì • ê²°ê³¼
            
        TODO for Chloe: ì‹¤ì œ PPO/DQN ì¶”ë¡  êµ¬í˜„
        """
        start_time = time.perf_counter()
        
        if self.ppo_model or self.dqn_model:
            # ì‹¤ì œ RL ëª¨ë¸ ì¶”ë¡ 
            result = self._real_rl_decision(game_state)
        else:
            # ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ
            result = self._simulate_decision(game_state)
        
        # ì„±ëŠ¥ ì¸¡ì •
        decision_time = time.perf_counter() - start_time
        self.decision_times.append(decision_time)
        self.action_history.append(result.action)
        
        return result
    
    def _simulate_decision(self, game_state: Dict[str, Any]) -> AIDecisionResult:
        """
        ì‹œë®¬ë ˆì´ì…˜ëœ AI ì˜ì‚¬ê²°ì • (í˜„ì¬ êµ¬í˜„)
        
        Chloeê°€ _real_rl_decision()ìœ¼ë¡œ êµì²´í•  ì˜ˆì •
        """
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹± ê¸°ë°˜ ì˜ì‚¬ê²°ì •
        player_y = game_state.get('player_y', 0.5)
        obstacle_y = game_state.get('obstacle_y', 0.0)
        obstacle_distance = game_state.get('obstacle_distance', 1.0)
        time_to_collision = game_state.get('time_to_collision', 10.0)
        
        # ì˜ì‚¬ê²°ì • ë¡œì§
        if time_to_collision < 1.0 and obstacle_distance < 0.3:
            if player_y > 0.7:  # í”Œë ˆì´ì–´ê°€ ì•„ë˜ìª½ì— ìˆìœ¼ë©´
                action = "jump"
                reasoning = "ì¥ì• ë¬¼ì´ ê°€ê¹Œì›Œì„œ ì í”„"
                confidence = 0.8
            else:
                action = "stay"
                reasoning = "ì´ë¯¸ ìœ„ìª½ì— ìˆì–´ì„œ ëŒ€ê¸°"
                confidence = 0.6
        else:
            # ëœë¤ í–‰ë™ (íƒí—˜)
            actions = ["stay", "jump", "left", "right"]
            weights = [0.4, 0.3, 0.15, 0.15]
            action = np.random.choice(actions, p=weights)
            reasoning = f"íƒí—˜ì  í–‰ë™: {action}"
            confidence = 0.5
        
        # í–‰ë™ í™•ë¥  ë¶„í¬ (ì‹œë®¬ë ˆì´ì…˜)
        action_probs = {
            "stay": 0.4,
            "jump": 0.3,
            "left": 0.15,
            "right": 0.15
        }
        action_probs[action] += 0.2  # ì„ íƒëœ í–‰ë™ì˜ í™•ë¥  ì¦ê°€
        
        return AIDecisionResult(
            action=action,
            confidence=confidence,
            reasoning=reasoning,
            action_probs=action_probs
        )
    
    def _real_rl_decision(self, game_state: Dict[str, Any]) -> AIDecisionResult:
        """
        ì‹¤ì œ ê°•í™”í•™ìŠµ ëª¨ë¸ ì˜ì‚¬ê²°ì •
        
        TODO for Chloe: ì´ í•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ì„¸ìš”!
        
        êµ¬í˜„ ê°€ì´ë“œ:
        1. ê²Œì„ ìƒíƒœë¥¼ RL ëª¨ë¸ ì…ë ¥ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        2. PPO ë˜ëŠ” DQN ì¶”ë¡  ì‹¤í–‰
        3. í–‰ë™ í™•ë¥  ë¶„í¬ ê³„ì‚°
        4. ìµœì  í–‰ë™ ì„ íƒ
        5. ì˜ì‚¬ê²°ì • ê·¼ê±° ìƒì„±
        """
        try:
            # ìƒíƒœ ë²¡í„° ìƒì„±
            state_vector = self._create_state_vector(game_state)
            
            if self.algorithm == "PPO" and self.ppo_model:
                # TODO: PPO ì¶”ë¡ 
                # action, _states = self.ppo_model.predict(state_vector, deterministic=False)
                # action_probs = self._get_action_probabilities(state_vector)
                
                # ì„ì‹œ: ì‹œë®¬ë ˆì´ì…˜ í˜¸ì¶œ
                return self._simulate_decision(game_state)
                
            elif self.algorithm == "DQN" and self.dqn_model:
                # TODO: DQN ì¶”ë¡ 
                # action, _states = self.dqn_model.predict(state_vector, deterministic=False)
                # q_values = self._get_q_values(state_vector)
                
                # ì„ì‹œ: ì‹œë®¬ë ˆì´ì…˜ í˜¸ì¶œ
                return self._simulate_decision(game_state)
            
        except Exception as e:
            print(f"âŒ RL ëª¨ë¸ ì¶”ë¡  ì˜¤ë¥˜: {e}")
            # ì˜¤ë¥˜ ì‹œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ í´ë°±
            return self._simulate_decision(game_state)
    
    def _create_state_vector(self, game_state: Dict[str, Any]) -> np.ndarray:
        """
        ê²Œì„ ìƒíƒœë¥¼ RL ëª¨ë¸ ì…ë ¥ ë²¡í„°ë¡œ ë³€í™˜
        
        TODO for Chloe: ìƒíƒœ í‘œí˜„ ìµœì í™”
        """
        # 8ì°¨ì› ìƒíƒœ ë²¡í„° ìƒì„±
        state_vector = np.array([
            game_state.get('player_x', 0.5),
            game_state.get('player_y', 0.5),
            game_state.get('player_vy', 0.0),
            game_state.get('on_ground', 0.0),
            game_state.get('obstacle_x', 0.0),
            game_state.get('obstacle_y', 0.0),
            game_state.get('obstacle_distance', 1.0),
            game_state.get('time_to_collision', 10.0)
        ], dtype=np.float32)
        
        return state_vector
    
    def update_reward(self, reward: float, done: bool = False):
        """
        ë³´ìƒ ì—…ë°ì´íŠ¸ (ìê°€ í•™ìŠµìš©)
        
        TODO for Chloe: ì˜¨ë¼ì¸ í•™ìŠµ êµ¬í˜„
        """
        self.reward_history.append(reward)
        
        if self.rl_logger:
            # TODO: RL ê³„ì¸¡ ì‹œìŠ¤í…œì— ê¸°ë¡
            # self.rl_logger.log_step(reward, done)
            pass
        
        # ì—í”¼ì†Œë“œ ì¢…ë£Œ ì‹œ í•™ìŠµ (ì„ íƒì )
        if done and len(self.reward_history) > 100:
            self._update_policy()
    
    def _update_policy(self):
        """
        ì •ì±… ì—…ë°ì´íŠ¸ (ì˜¨ë¼ì¸ í•™ìŠµ)
        
        TODO for Chloe: PPO/DQN ì˜¨ë¼ì¸ í•™ìŠµ êµ¬í˜„
        """
        # TODO: ì‹¤ì œ ì •ì±… ì—…ë°ì´íŠ¸ êµ¬í˜„
        # 1. ê²½í—˜ ë²„í¼ì—ì„œ ë°°ì¹˜ ìƒ˜í”Œë§
        # 2. ì •ì±… ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        # 3. ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        # 4. ì„±ëŠ¥ ë¡œê¹…
        
        print("ğŸ”„ [Chloe TODO] ì •ì±… ì—…ë°ì´íŠ¸ ì‹¤í–‰")
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        if not self.decision_times:
            return {}
        
        avg_decision_time = np.mean(self.decision_times)
        avg_reward = np.mean(self.reward_history) if self.reward_history else 0
        
        # í–‰ë™ ë¶„í¬ ê³„ì‚°
        action_counts = {}
        for action in self.action_history:
            action_counts[action] = action_counts.get(action, 0) + 1
        
        return {
            'avg_decision_time_ms': avg_decision_time * 1000,
            'avg_reward': avg_reward,
            'total_decisions': len(self.action_history),
            'action_distribution': action_counts,
            'recent_actions': self.action_history[-10:],  # ìµœê·¼ 10ê°œ í–‰ë™
            'algorithm': self.algorithm
        }
    
    def reset_episode(self):
        """ì—í”¼ì†Œë“œ ì´ˆê¸°í™”"""
        if self.rl_logger:
            # TODO: ì—í”¼ì†Œë“œ ì¢…ë£Œ ë¡œê¹…
            # self.rl_logger.log_episode_end(...)
            pass
        
        # íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™” (ì„ íƒì )
        if len(self.action_history) > 1000:  # ë©”ëª¨ë¦¬ ê´€ë¦¬
            self.action_history = self.action_history[-500:]
            self.reward_history = self.reward_history[-500:]
    
    def save_model(self, save_path: str):
        """
        ëª¨ë¸ ì €ì¥
        
        TODO for Chloe: í›ˆë ¨ëœ ëª¨ë¸ ì €ì¥ êµ¬í˜„
        """
        if self.ppo_model:
            self.ppo_model.save(save_path)
        elif self.dqn_model:
            self.dqn_model.save(save_path)
        else:
            # PyTorch ëª¨ë¸ ì €ì¥
            if TORCH_AVAILABLE and self.policy_net:
                torch.save(self.policy_net.state_dict(), save_path)
        
        print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {save_path}")


# Chloeê°€ ì‚¬ìš©í•  í—¬í¼ í•¨ìˆ˜ë“¤
def create_reward_function(game_state: Dict[str, Any], action: str, next_state: Dict[str, Any]) -> float:
    """
    ë³´ìƒ í•¨ìˆ˜ ì„¤ê³„
    
    TODO for Chloe: ê²Œì„ì— ë§ëŠ” ë³´ìƒ í•¨ìˆ˜ êµ¬í˜„
    """
    reward = 0.0
    
    # ìƒì¡´ ë³´ìƒ
    if not next_state.get('game_over', False):
        reward += 1.0
    
    # ì¶©ëŒ í˜ë„í‹°
    if next_state.get('game_over', False):
        reward -= 100.0
    
    # ì ìˆ˜ ì¦ê°€ ë³´ìƒ
    score_diff = next_state.get('score', 0) - game_state.get('score', 0)
    reward += score_diff * 10.0
    
    # ë¶ˆí•„ìš”í•œ í–‰ë™ í˜ë„í‹° (ì„ íƒì )
    if action in ["left", "right"] and game_state.get('obstacle_distance', 1.0) > 0.5:
        reward -= 0.1
    
    return reward


def analyze_failure_mode(game_state: Dict[str, Any], action: str) -> str:
    """
    ì‹¤íŒ¨ ëª¨ë“œ ë¶„ì„
    
    Chloeê°€ ë””ë²„ê¹…ìš©ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜
    """
    if game_state.get('game_over', False):
        obstacle_distance = game_state.get('obstacle_distance', 1.0)
        time_to_collision = game_state.get('time_to_collision', 10.0)
        
        if obstacle_distance < 0.2 and action == "stay":
            return "íšŒí”¼ ì‹¤íŒ¨: ì¥ì• ë¬¼ì´ ê°€ê¹Œìš´ë° í–‰ë™í•˜ì§€ ì•ŠìŒ"
        elif time_to_collision < 0.5 and action in ["left", "right"]:
            return "ì˜ëª»ëœ íšŒí”¼: ì í”„ ëŒ€ì‹  ì¢Œìš° ì´ë™"
        else:
            return "ì¼ë°˜ì ì¸ ì¶©ëŒ"
    
    return "ì •ìƒ"


# ì‚¬ìš© ì˜ˆì‹œ (Chloeê°€ ì°¸ê³ í•  ì½”ë“œ)
if __name__ == "__main__":
    # AI ëª¨ë“ˆ ì´ˆê¸°í™”
    ai_module = AIModule(
        model_path="path/to/ppo_model.zip",  # Chloeê°€ í›ˆë ¨í•œ ëª¨ë¸
        algorithm="PPO"
    )
    
    # í…ŒìŠ¤íŠ¸ ê²Œì„ ìƒíƒœ
    test_state = {
        'player_x': 0.5,
        'player_y': 0.8,
        'player_vy': 0.0,
        'on_ground': 1.0,
        'obstacle_x': 0.6,
        'obstacle_y': 0.3,
        'obstacle_distance': 0.4,
        'time_to_collision': 2.0
    }
    
    # AI ì˜ì‚¬ê²°ì •
    decision = ai_module.make_decision(test_state)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"ì„ íƒëœ í–‰ë™: {decision.action}")
    print(f"ì‹ ë¢°ë„: {decision.confidence:.2f}")
    print(f"ê·¼ê±°: {decision.reasoning}")
    
    # ì„±ëŠ¥ í†µê³„
    stats = ai_module.get_performance_stats()
    print(f"í‰ê·  ì˜ì‚¬ê²°ì • ì‹œê°„: {stats.get('avg_decision_time_ms', 0):.1f}ms")
