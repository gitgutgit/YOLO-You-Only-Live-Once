# 🎬 데모 영상 대본 (논문 싱크 버전)

**총 길이**: 약 7-8분  
**스타일**: 논문을 보여주면서 설명하는 형식

---

## [0:00 - 0:30] 타이틀 & 팀 소개

**[화면: 논문 표지 페이지 전체]**

> 안녕하세요. 저희는 Columbia University COMS W4995 Deep Learning for Computer Vision 수업의 Team Prof.Peter.backward()입니다.
>
> 오늘 발표할 프로젝트는 "Distilled Vision Agent: YOLO, You Only Live Once"입니다.
>
> 이 프로젝트는 순수 시각 정보만으로 2D 게임을 플레이하는 딥러닝 에이전트를 개발한 것입니다.

---

## [0:30 - 1:00] Abstract 요약

**[화면: 논문 Abstract 섹션 (왼쪽) + 시스템 아키텍처 다이어그램 (오른쪽)]**

> 먼저 Abstract를 보시면, 저희의 핵심 기여사항을 확인하실 수 있습니다.
>
> 첫째, YOLOv8-nano 객체 탐지와 PPO 강화학습을 결합한 end-to-end 파이프라인을 구축했습니다.
>
> 둘째, 1,465개의 게임 프레임을 직접 수집하고 라벨링하여 커스텀 데이터셋을 만들었습니다.
>
> 셋째, 웹 애플리케이션으로 배포하여 실시간으로 데이터를 수집하고 에이전트를 평가할 수 있습니다.
>
> 결과적으로, 98.8% mAP@50의 객체 탐지 정확도와 22.8%의 생존 시간 향상을 달성했습니다.

---

## [1:00 - 2:00] Introduction

**[화면: 논문 Introduction 섹션 (왼쪽) + 게임 스크린샷 (오른쪽)]**

> Introduction 섹션으로 넘어가면, 저희가 해결하려는 문제를 확인하실 수 있습니다.
>
> 전통적인 게임 AI는 게임 엔진의 내부 상태에 직접 접근합니다. 하지만 실제 로봇이나 자율주행 시스템은 그렇지 않죠. 오직 시각 정보만으로 판단해야 합니다.
>
> 저희 에이전트는 사람처럼 게임 화면을 보고, 객체를 탐지하고, 상태를 추출한 다음, 행동을 결정합니다.
>
> 학습 과정은 두 단계로 나뉩니다. 첫 번째는 Policy Distillation으로, 전문가의 시연을 모방합니다. 두 번째는 PPO 강화학습으로, 전문가를 넘어서는 성능을 달성합니다.

---

## [2:00 - 3:00] System Architecture

**[화면: 논문 Methodology 섹션의 System Architecture 부분 (왼쪽) + 파이프라인 다이어그램 (오른쪽)]**

> Methodology 섹션의 System Architecture를 보시면, 저희 시스템의 전체 구조를 확인하실 수 있습니다.
>
> 시스템은 4가지 주요 컴포넌트로 구성됩니다.
>
> 첫째, Pygame 기반의 2D 사이드 스크롤 생존 게임입니다. 플레이어는 운석을 피하고 별을 수집해야 합니다.
>
> 둘째, YOLOv8-nano 객체 탐지 모듈입니다. RGB 프레임을 입력받아 바운딩 박스를 출력합니다.
>
> 셋째, State Encoder입니다. 탐지 결과를 구조화된 상태 벡터로 변환합니다. 플레이어 위치, 속도, 장애물까지의 거리, 갭의 기하학적 정보 등이 포함됩니다.
>
> 넷째, 3층 MLP 정책 네트워크입니다. 상태 벡터를 입력받아 점프, 대기, 좌우 이동 등의 행동을 결정합니다.
>
> 전체 루프는 다음과 같습니다: RGB 프레임 캡처 → YOLO 탐지 → 상태 인코딩 → 정책 네트워크 → 행동 적용. 이 과정이 실시간으로 반복됩니다.

---

## [3:00 - 4:30] YOLO Detection

**[화면: 논문 "Object Detection Training" 섹션 (왼쪽) + YOLO 라벨링 예시 (오른쪽)]**

> Object Detection Training 섹션을 보시면, YOLO 훈련 과정을 확인하실 수 있습니다.
>
> 저희는 웹 플랫폼에서 수집한 게임플레이 데이터로부터 1,465개의 프레임을 선별하고 라벨링했습니다.
>
> 4가지 클래스를 정의했습니다: 플레이어, 운석, 별, 그리고 용암 경고입니다.
>
> YOLOv8-nano를 50 에포크 동안 훈련했습니다. 사전 훈련된 COCO 모델을 기반으로 fine-tuning했습니다.

**[화면 전환: 훈련 곡선 그래프]**

> 훈련 곡선을 보시면, mAP@50이 에포크가 진행될수록 증가하는 것을 확인할 수 있습니다.
>
> 최종적으로 98.8%의 mAP@50을 달성했습니다. 이는 목표였던 70%를 크게 초과하는 결과입니다.

**[화면 전환: 검증 결과 이미지]**

> 검증 결과를 보시면, YOLO가 모든 객체를 정확하게 탐지하는 것을 확인할 수 있습니다.
>
> 플레이어는 99.1%, 운석은 98.5%, 별은 97.8%, 용암 경고는 98.2%의 mAP@50을 달성했습니다.
>
> 이 높은 탐지 정확도는 이후 정책 네트워크가 정확한 상태 정보를 받을 수 있게 해줍니다.

---

## [4:30 - 5:30] Policy Distillation

**[화면: 논문 "Policy Distillation" 섹션 (왼쪽) + Human Mode 게임플레이 (오른쪽)]**

> Policy Distillation 섹션으로 넘어가면, 첫 번째 학습 단계를 확인하실 수 있습니다.
>
> 저희는 웹 플랫폼의 Human Mode에서 전문가 플레이어들의 게임플레이를 수집했습니다.
>
> 각 프레임마다 YOLO 탐지 결과로부터 상태 벡터를 추출하고, 전문가가 취한 행동을 기록했습니다.
>
> 이렇게 수집된 State-Action 쌍으로 3층 MLP를 지도학습했습니다.

**[화면 전환: State-Action 쌍 시각화]**

> 예를 들어, 플레이어가 장애물에 가까워지면 전문가는 점프를 선택합니다.
>
> 정책 네트워크는 이런 패턴을 학습하여, 유사한 상황에서 전문가와 같은 행동을 선택하도록 훈련됩니다.
>
> 결과적으로, 78.3%의 action agreement를 달성했습니다. 이는 목표였던 75%를 초과하는 결과입니다.
>
> 이 Distilled Policy는 강화학습의 안정적인 시작점이 됩니다.

---

## [5:30 - 7:00] PPO Reinforcement Learning

**[화면: 논문 "Reinforcement Learning Fine-Tuning" 섹션 (왼쪽) + PPO 훈련 곡선 (오른쪽)]**

> Reinforcement Learning Fine-Tuning 섹션을 보시면, 두 번째 학습 단계를 확인하실 수 있습니다.
>
> Proximal Policy Optimization, 즉 PPO 알고리즘을 사용했습니다.
>
> Distilled Policy를 초기화로 사용하여, self-play를 통해 정책을 개선했습니다.
>
> 보상 신호는 다음과 같이 설계했습니다: 생존할 때마다 +1, 충돌 시 -10, 별 수집 시 +5입니다.

**[화면 전환: TensorBoard 스크린샷]**

> TensorBoard에서 훈련 곡선을 보시면, 평균 생존 시간이 에피소드가 진행될수록 증가하는 것을 확인할 수 있습니다.
>
> 200 에피소드 이상의 self-play를 통해, 에이전트는 전문가 시연을 넘어서는 전략을 학습했습니다.

**[화면 전환: AI Mode 게임플레이]**

> 실제 게임플레이를 보시면, 에이전트가 장애물을 미리 예측하고 점프 타이밍을 최적화하는 것을 확인할 수 있습니다.
>
> 또한 별을 수집하면서도 안전을 유지하는 균형잡힌 전략을 보여줍니다.

---

## [7:00 - 8:30] Results & Demo

**[화면: 논문 "Experiments and Results" 섹션 (왼쪽) + 게임플레이 비교 영상 (오른쪽)]**

> Experiments and Results 섹션을 보시면, 저희의 성과를 확인하실 수 있습니다.
>
> 모든 성공 기준을 달성했습니다.
>
> 첫째, 객체 탐지: 98.8% mAP@50 (목표 70% 초과)
>
> 둘째, 모방 정확도: 78.3% action agreement (목표 75% 초과)
>
> 셋째, 성능 향상: 22.8% 생존 시간 개선 (목표 20% 초과)
>
> 넷째, 실시간 성능: 12.9ms per frame, 즉 77.5 FPS (목표 60 FPS 초과)

**[화면 전환: 게임플레이 비교 (Random vs Distilled vs PPO)]**

> 게임플레이 비교를 보시면, 세 가지 방법의 차이를 명확하게 확인할 수 있습니다.
>
> Random Policy는 거의 즉시 충돌합니다.
>
> Distilled Policy는 기본적인 생존 전략을 보여줍니다.
>
> PPO Fine-tuned Agent는 가장 오래 생존하며, 복잡한 장애물 패턴도 처리합니다.

**[화면 전환: 실시간 FPS 표시]**

> 실시간 성능을 보시면, 전체 파이프라인이 12.9ms 내에 완료되는 것을 확인할 수 있습니다.
>
> YOLO 탐지가 8.3ms, 정책 네트워크가 0.8ms로, 실시간 게임플레이에 충분히 빠릅니다.

**[화면: 실패 케이스 분석]**

> 물론 완벽하지는 않습니다. 주요 실패 케이스는 다음과 같습니다.
>
> 첫째, 빠른 연속 장애물: 에이전트가 반응할 시간이 부족합니다.
>
> 둘째, 좁은 갭: 정확한 타이밍이 필요한 경우 실패할 수 있습니다.
>
> 셋째, 객체 겹침: 탐지 모호성이 발생할 수 있습니다.
>
> 이러한 한계는 향후 연구에서 개선할 수 있습니다.

---

## [8:30 - 9:00] Conclusion

**[화면: 논문 Conclusion 섹션 (전체)]**

> Conclusion 섹션을 보시면, 저희의 핵심 성과를 요약할 수 있습니다.
>
> 저희는 순수 시각 정보만으로 게임을 플레이하는 딥러닝 에이전트를 성공적으로 개발했습니다.
>
> YOLO 객체 탐지, Policy Distillation, 그리고 PPO 강화학습을 통합하여, 모든 성공 기준을 달성했습니다.
>
> 이 프로젝트는 데이터 수집부터 모델 훈련, 배포까지의 전체 파이프라인을 보여주며, 실제 작동하는 시스템을 구축했습니다.

**[화면 전환: GitHub 링크 + 웹 플랫폼 URL]**

> 웹 플랫폼은 현재도 운영 중이며, 전 세계 플레이어들로부터 데이터를 수집하고 있습니다.
>
> GitHub 저장소와 웹 플랫폼 URL은 논문에 포함되어 있습니다.
>
> 감사합니다.

---

## 📝 나레이션 팁

### 속도 조절

- **천천히**: 기술 용어는 특히 명확하게
- **일관성**: 전체 영상에서 비슷한 속도 유지
- **간격**: 논문 설명 후 짧은 멈춤 (시청자가 읽을 시간)

### 강조 포인트

- **숫자**: "98.8%", "22.8%", "77.5 FPS" 등은 명확하게
- **섹션 참조**: "Section 3.2를 보시면..." 같은 참조
- **시각적 요소**: "화면을 보시면..." 같은 안내

### 톤

- **전문적**: 학술적이지만 친근하게
- **자신감**: 성과를 명확하게 제시
- **솔직함**: 한계도 인정

---

## 🎬 편집 노트

### 화면 전환 타이밍

- 논문 설명 후 2-3초 멈춤
- 이미지/영상 전환 시 부드러운 fade
- 중요한 숫자는 텍스트 오버레이로 강조

### 자막/텍스트 오버레이

- 기술 용어: YOLO, PPO, mAP 등
- 핵심 숫자: 98.8%, 22.8% 등
- 섹션 번호: "Section 3.2" 등

### 배경 음악 (선택)

- 매우 낮은 볼륨
- 전자적/미래지향적 느낌
- 나레이션을 방해하지 않도록

---

**작성일**: 2024-12-01  
**작성자**: Team Prof.Peter.backward()
